{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LpCKZtdqZAgj",
        "k1NbBXyWZD7C",
        "NwlQaQlVmRcy",
        "gHb00g_6mTRS",
        "7zqg5F8Ll9BO",
        "SBaTJhefmM8l"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# params"
      ],
      "metadata": {
        "id": "prmi14SE-8Hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_root_path = \"/content/drive/Shareddrives/Curie/benchmarks/public_release\" #@param"
      ],
      "metadata": {
        "id": "1klrqpcq_Auz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# read files in drive"
      ],
      "metadata": {
        "id": "LpCKZtdqZAgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9-OqbH2Yktq",
        "outputId": "ea0220a7-e1a3-424c-e215-cfb2f7791864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install"
      ],
      "metadata": {
        "id": "k1NbBXyWZD7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install json5"
      ],
      "metadata": {
        "id": "hah1T1Yx5tY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "_26XVvwZRREs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r rouge/requirements.txt\n",
        "!pip install rouge-score\n",
        "!pip install Bio\n",
        "!pip install Levenshtein"
      ],
      "metadata": {
        "id": "CXhrgXJ7U7DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "rXI7uAyl7cH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json5\n",
        "import re\n",
        "import functools\n",
        "from typing import Any, Tuple, Union, Dict\n",
        "from Bio import Align\n",
        "import glob\n",
        "from bert_score import score\n",
        "from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "import ast\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import Levenshtein"
      ],
      "metadata": {
        "id": "VchTG3815jUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# eval functions"
      ],
      "metadata": {
        "id": "iRmqK0PU7Pv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMSim eval"
      ],
      "metadata": {
        "id": "sH5WLIFdGNDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "94xUwOLFNmQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLMSim util"
      ],
      "metadata": {
        "id": "v5-94GHgHW4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@functools.lru_cache(maxsize=1)\n",
        "def get_model(model_name: str = 'gemini-1.5-pro-latest'):\n",
        "\n",
        "  return genai.GenerativeModel(model_name=model_name)\n",
        "\n",
        "\n",
        "def llm_output(client: Any, prompt: str) -> str:\n",
        "  # client=None for external api\n",
        "  return get_model().generate_content(prompt).text\n",
        "\n",
        "def model_eval_json(\n",
        "    record_id: str | None,\n",
        "    json_ground_truth: list[dict[str, Any]],\n",
        "    json_model_response: list[dict[str, Any]],\n",
        "    eval_prompt: str,\n",
        "    client: Any,\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Evaluate with json ground truth and model response.\"\"\"\n",
        "  eval_list = []\n",
        "  genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "  for j, ground_truth_item in enumerate(json_ground_truth):\n",
        "    for k, model_response_item in enumerate(json_model_response):\n",
        "      # Add index to llm input to suppress hallucination on json_extracted_index\n",
        "      # prediction\n",
        "      model_response_item[\"json_extracted_index\"] = k\n",
        "    prompt = (\n",
        "        load_matsci_prompt(eval_prompt)\n",
        "        .replace(\n",
        "            \"{{json_ground_truth}}\", json5.dumps(ground_truth_item, indent=2)\n",
        "        )\n",
        "        .replace(\n",
        "            \"{{json_extracted_list}}\", json5.dumps(json_model_response, indent=2)\n",
        "        )\n",
        "    )\n",
        "    output = llm_output(client=client, prompt=prompt)\n",
        "    try:\n",
        "      output_json = json5.loads(output)\n",
        "    except Exception as e:  # pylint: disable=broad-except\n",
        "      print(\"Skipping incomplete last item in output: \", e)\n",
        "      inds = [m.start() for m in re.finditer(r\",\\s*\\{\", output)]\n",
        "      if inds:\n",
        "        ind = inds[-1]\n",
        "        output_json = json5.loads(output[:ind] + \"]\")\n",
        "      else:\n",
        "        output_json = []\n",
        "    if isinstance(output_json, list):\n",
        "      # Handle edge case that model hallucinated outputing list enclosing json.\n",
        "      if not output_json:\n",
        "        output_json = {}\n",
        "      else:\n",
        "        output_json = output_json[0]\n",
        "    output_json[\"json_ground_truth_index\"] = j\n",
        "    output_json[\"json_ground_truth\"] = ground_truth_item\n",
        "    output_json[\"json_extracted\"] = {}\n",
        "    # If not in it, it means llm didn't find a good match, so leave it empty.\n",
        "    if \"json_extracted_index\" in output_json:\n",
        "      if (\n",
        "          str(output_json[\"json_extracted_index\"]).isdigit()\n",
        "          and int(output_json[\"json_extracted_index\"]) > 0\n",
        "          and int(output_json[\"json_extracted_index\"])\n",
        "          < len(json_model_response)\n",
        "      ):\n",
        "        output_json[\"json_extracted\"] = json_model_response[\n",
        "            int(output_json[\"json_extracted_index\"])\n",
        "        ]\n",
        "      else:\n",
        "        del output_json[\"json_extracted_index\"]\n",
        "    eval_list.append(output_json)\n",
        "  return {\n",
        "      \"record_id\": record_id,\n",
        "      \"ground_truth_length\": len(json_ground_truth),\n",
        "      \"model_response_length\": len(json_model_response),\n",
        "      \"response_json\": eval_list,\n",
        "  }"
      ],
      "metadata": {
        "id": "N10Sp_IDHal6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLMSim dft"
      ],
      "metadata": {
        "id": "Z3q-IXimHghk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lmsim_score_dft(prediction, reference):\n",
        "  return dft_domain_expert_model_based_eval(reference, prediction, client=None) # Client=None for external api\n",
        "\n",
        "\n",
        "_METADATA_EVAL_PROMPT_FILENAME = file_root_path + \"/prompts/dft_metadata_eval_output_1_shot.txt\"\n",
        "_STRUCTURE_EVAL_PROMPT_FILENAME = file_root_path + \"/prompts/dft_structure_eval_output_1_shot.txt\"\n",
        "\n",
        "\n",
        "def get_dft_model_response_field(\n",
        "    model_output_value: dict[str, Any], field_name: str\n",
        ") -> list[Any] | str:\n",
        "  \"\"\"Returns the model response for a given field from the inference output.\n",
        "\n",
        "     This applies to json responses from the dft chained inference output.\n",
        "\n",
        "  Args:\n",
        "    model_output_value: The model output response (from one two) as a dict.\n",
        "    field_name: The name of the field to extract. This should be one of\n",
        "      \"structure_metadata\", \"dft_metadata\", or \"code\".\n",
        "  \"\"\"\n",
        "  if field_name in [\"structure_metadata\"]:\n",
        "    if field_name in model_output_value:\n",
        "      print(model_output_value)\n",
        "      return model_output_value[\"structure_metadata\"]\n",
        "    else:\n",
        "      return \"\"\n",
        "\n",
        "  elif field_name in [\"dft_metadata\"]:\n",
        "    if field_name in model_output_value:\n",
        "      return model_output_value[\"dft_metadata\"]\n",
        "    else:\n",
        "      return \"\"\n",
        "\n",
        "  elif field_name == \"code\":\n",
        "    if field_name in model_output_value:\n",
        "      code = \"\\n\".join(\n",
        "          [x[\"code_element\"] for x in model_output_value[\"code_elements\"]]\n",
        "      )\n",
        "      code += \"\\n\" + model_output_value[\"execution_code\"]\n",
        "      return code\n",
        "    else:\n",
        "      return \"\"\n",
        "\n",
        "  raise ValueError(f\"Unknown field name: {field_name}\")\n",
        "\n",
        "\n",
        "def get_annotated_structure_metadata_and_dft_params(\n",
        "    gt_paper_code: str, verbose: int = 0\n",
        ") -> dict[str, list[str]]:\n",
        "  \"\"\"Returns structure metadata and dft params from the ground truth code.\n",
        "\n",
        "  Args:\n",
        "    gt_paper_code: The ground truth code from .py file as a string.\n",
        "    verbose: The verbosity level.\n",
        "  \"\"\"\n",
        "  structures = []\n",
        "  dft_params = []\n",
        "\n",
        "  paper = gt_paper_code\n",
        "  if \"structure_metadata_\" in paper:\n",
        "    parts = paper.split(\"structure_metadata_\")[1:]\n",
        "    whole_parts = [\"structure_metadata_\" + part for part in parts]\n",
        "\n",
        "    for part in whole_parts:\n",
        "      if verbose > 1:\n",
        "        print(\"PART:\\n\", part)\n",
        "      if \"parse_raw(\" not in part:\n",
        "        continue\n",
        "      left, right, *_ = part.split(\"parse_raw(\")\n",
        "      if \"StructureMetadata\" in left:\n",
        "        end_struc = \")\"\n",
        "        if \"')\" in right:\n",
        "          end_struc = \"')\"\n",
        "        elif \"'\\n)\" in right:\n",
        "          end_struc = \"'\\n)\"\n",
        "        struc_json = right.split(end_struc)[0].strip()\n",
        "        if verbose > 0:\n",
        "          print(\"Extracted structure:\\n\", struc_json)\n",
        "        # clean_json = struc_json.replace('NaN', '\"NaN\"')\n",
        "        # struc_json = ast.literal_eval(clean_json)\n",
        "        structures.append(struc_json)\n",
        "\n",
        "  if \"dft_params_\" in paper:\n",
        "    parts = paper.split(\"dft_params_\")[1:]\n",
        "    # print(parts)\n",
        "    whole_parts = [\"dft_params_\" + part for part in parts]\n",
        "\n",
        "    for part in whole_parts:\n",
        "      if verbose > 1:\n",
        "        print(\"PART:\\n\", part)\n",
        "      if \"parse_raw(\" not in part:\n",
        "        continue\n",
        "      left, right, *_ = part.split(\"parse_raw(\")\n",
        "      if \"DFTParameters\" in left:\n",
        "        end_struc = \")\"\n",
        "        if \"')\" in right:\n",
        "          end_struc = \"')\"\n",
        "        elif \"'\\n)\" in right:\n",
        "          end_struc = \"'\\n)\"\n",
        "        dft_params_str = right.split(end_struc)[0].strip()\n",
        "        if verbose > 0:\n",
        "          print(\"Extracted dft_param:\\n\", dft_params_str)\n",
        "        # clean_json = dft_params_str.replace('NaN', '\"NaN\"')\n",
        "        # dft_params_str = ast.literal_eval(clean_json)\n",
        "        dft_params.append(dft_params_str)\n",
        "\n",
        "  gt_struc_jsons = []\n",
        "  for struct_metadata in structures:\n",
        "    gt_json = struct_metadata.split(\"'\")[1]\n",
        "    clean_json = gt_json.replace(\"NaN\", '\"NaN\"')\n",
        "    try:\n",
        "      gt_structure_json = ast.literal_eval(clean_json)\n",
        "      gt_struc_jsons.append(gt_structure_json)\n",
        "    except Exception:  # pylint: disable=broad-exception-caught\n",
        "      gt_struc_jsons.append(clean_json)\n",
        "\n",
        "  gt_dft_params_jsons = []\n",
        "  for dft_param in dft_params:\n",
        "    gt_json = dft_param.split(\"'\")[1]\n",
        "    clean_json = gt_json.replace(\"NaN\", '\"NaN\"')\n",
        "    try:\n",
        "      gt_dft_json = ast.literal_eval(clean_json)\n",
        "      gt_dft_params_jsons.append(gt_dft_json)\n",
        "    except Exception:  # pylint: disable=broad-exception-caught\n",
        "      gt_dft_params_jsons.append(clean_json)\n",
        "\n",
        "  return {\n",
        "      \"structures_metadata\": gt_struc_jsons,\n",
        "      \"dft_params\": gt_dft_params_jsons,\n",
        "  }\n",
        "\n",
        "\n",
        "def get_material_composition_from_struc(\n",
        "    structure: str | dict[str, str],\n",
        ") -> str | None:\n",
        "  \"\"\"Returns material composition from the structure metadata dict or string.\"\"\"\n",
        "  if isinstance(structure, dict):\n",
        "    if \"composition\" in structure:\n",
        "      return structure[\"composition\"]\n",
        "  elif isinstance(structure, str):\n",
        "    if \"composition\" in structure:\n",
        "      material = structure.split(r\"\\\"composition\\\":\")[1].split(\",\")[0]\n",
        "      return material\n",
        "  return None\n",
        "\n",
        "\n",
        "def get_json_from_str(input_str: str) -> dict[str, Any] | None:\n",
        "  \"\"\"Returns the json object from the ground truth input string.\"\"\"\n",
        "  output_val = input_str.replace(\"NaN\", '\"NaN\"')\n",
        "  output_val = output_val.replace(\"true\", '\"1.0\"')\n",
        "  output_val = output_val.replace(\"false\", '\"NaN\"')\n",
        "  try:\n",
        "    output_val = ast.literal_eval(output_val)\n",
        "  except ValueError:\n",
        "    return None\n",
        "  return output_val\n",
        "\n",
        "\n",
        "def parse_ground_truth_dft(ground_truth: str, client: Any) -> list[dict[str, Any]]:\n",
        "  \"\"\"Parses ground truth.\"\"\"\n",
        "  try:\n",
        "    json_ground_truth = json5.loads(\n",
        "        ground_truth.replace(\"\\n\", \"\").replace(\"\\\\\", \"\")\n",
        "    )\n",
        "    if json_ground_truth and isinstance(json_ground_truth[0], str):\n",
        "      json_ground_truth = [json5.loads(item) for item in json_ground_truth]\n",
        "  except Exception:  # pylint: disable=broad-except\n",
        "    print(\"***using llm to parse\")\n",
        "    ground_truth = llm_output(\n",
        "        client,\n",
        "        prompt=\"Extract ground truth json list from the following text.\\n\"\n",
        "        + ground_truth\n",
        "        + \"\\nMake sure to remove all backslashes for escape characters. Output\"\n",
        "        \" the json list ONLY, without any explanation, prefix or suffix:\\n\",\n",
        "    )\n",
        "    print(\"***llm_ground_truth:\\n\", ground_truth)\n",
        "    json_ground_truth = json5.loads(\n",
        "        ground_truth.replace(\"\\n\", \"\").replace('\\\\\"', \"\").replace(\"\\\\\", \"\")\n",
        "    )\n",
        "\n",
        "  return json_ground_truth\n",
        "\n",
        "\n",
        "def parse_model_response_dft(\n",
        "    model_response: str, client: Any, use_llm=False\n",
        ") -> list[dict[str, Any]]:\n",
        "  \"\"\"Parses model response.\"\"\"\n",
        "  def remove_prefix_suffix(text):\n",
        "    return text.replace(\"\\n\", \"\").removeprefix(\"```json\").removesuffix(\"```json\").removeprefix(\"```\").removesuffix(\"```\").removeprefix(\"`\").removesuffix(\"`\")\n",
        "  if use_llm:\n",
        "    model_response = llm_output(\n",
        "            client,\n",
        "            prompt=\"Extract model_response json list from the following text.\\n\"\n",
        "            + model_response\n",
        "            + '\\nMake sure all None values are converted to \"NaN\". Output the json'\n",
        "            \" list\"\n",
        "            \" ONLY, without any explanation, prefix or suffix:\\n\",\n",
        "          )\n",
        "  response_text = remove_prefix_suffix(model_response)\n",
        "  try:\n",
        "    try:\n",
        "      formatted_text = re.sub(r\"(?<=\\w)'(?=\\w|\\s)\", \"\\\\'\", response_text)\n",
        "      if not formatted_text:\n",
        "        formatted_text = \"{}\"\n",
        "        print(\"Response_text is empty\")\n",
        "      json_model_response = json5.loads(formatted_text)\n",
        "    except Exception as e:  # pylint: disable=broad-except\n",
        "      print(\"Skipping incomplete last item: \", e)\n",
        "      print(\"***\", response_text)\n",
        "      ind = [m.start() for m in re.finditer(r\",\\s*\\{\", response_text)][-1]\n",
        "      json_model_response = json5.loads(response_text[:ind] + \"]\")\n",
        "  except:\n",
        "      return parse_model_response_dft(model_response, client, use_llm=True) if not use_llm else []\n",
        "  return json_model_response\n",
        "\n",
        "\n",
        "def dft_model_eval_paper(\n",
        "    record_id: str | None,\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    eval_prompt: str,\n",
        "    client: Any,\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Runs model evaluation on material properties for a single paper.\n",
        "\n",
        "  Args:\n",
        "    record_id: record id or paper id.\n",
        "    ground_truth: ground truth list in str type.\n",
        "    model_response: model response list in str type.\n",
        "    eval_prompt: eval prompt.\n",
        "    client: llm client.\n",
        "\n",
        "  Returns:\n",
        "    model eval response json.\n",
        "  \"\"\"\n",
        "  json_ground_truth = parse_ground_truth_dft(ground_truth, client)\n",
        "  json_model_response = parse_model_response_dft(model_response, client)\n",
        "  return model_eval_json(\n",
        "      record_id=record_id,\n",
        "      json_ground_truth=json_ground_truth,\n",
        "      json_model_response=json_model_response,\n",
        "      eval_prompt=eval_prompt,\n",
        "      client=client,\n",
        "  )\n",
        "\n",
        "\n",
        "def dft_metadata_domain_expert_model_based_eval(\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    client: Any | None = None,\n",
        "    eval_prompt: str = _METADATA_EVAL_PROMPT_FILENAME,\n",
        "    verbose: bool = True,\n",
        ") -> dict[str, Any]:\n",
        "  return dft_domain_expert_model_based_eval(\n",
        "      ground_truth=ground_truth,\n",
        "      model_response=model_response,\n",
        "      client=client,\n",
        "      eval_prompt=eval_prompt,\n",
        "      verbose=verbose,\n",
        "  )\n",
        "\n",
        "\n",
        "def dft_structure_domain_expert_model_based_eval(\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    client: Any | None = None,\n",
        "    eval_prompt: str = _STRUCTURE_EVAL_PROMPT_FILENAME,\n",
        "    verbose: bool = True,\n",
        ") -> dict[str, Any]:\n",
        "  return dft_domain_expert_model_based_eval(\n",
        "      ground_truth=ground_truth,\n",
        "      model_response=model_response,\n",
        "      client=client,\n",
        "      eval_prompt=eval_prompt,\n",
        "      verbose=verbose,\n",
        "  )\n",
        "\n",
        "\n",
        "def dft_domain_expert_model_based_eval(\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    client: Any | None = None,\n",
        "    eval_prompt: str = _METADATA_EVAL_PROMPT_FILENAME,\n",
        "    verbose: bool = True,\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Runs model based eval on dft.\n",
        "\n",
        "  Args:\n",
        "    ground_truth: ground truth list in str type.\n",
        "    model_response: model response list in str type.\n",
        "    client: llm client.\n",
        "    eval_prompt: eval prompt.\n",
        "    verbose: whether to print out eval results.\n",
        "\n",
        "  Returns:\n",
        "    eval result.\n",
        "  \"\"\"\n",
        "  if verbose:\n",
        "    print(\"Model eval started...\")\n",
        "  eval_output_item = dft_model_eval_paper(\n",
        "      record_id=None,\n",
        "      ground_truth=ground_truth,\n",
        "      model_response=model_response,\n",
        "      eval_prompt=eval_prompt,\n",
        "      client=client,\n",
        "  )\n",
        "  if verbose:\n",
        "    print(\"Model eval finished.\")\n",
        "  eval_result = eval_overall_result(\n",
        "      eval_output_item, verbose=verbose\n",
        "  )\n",
        "  if verbose:\n",
        "    print(\"Eval results:\\n\", eval_result)\n",
        "  return eval_result"
      ],
      "metadata": {
        "id": "cZpvEbZMHpD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLMSim mpve"
      ],
      "metadata": {
        "id": "pukWSU7WHwQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lmsim_score_mpve(prediction, reference):\n",
        "  # TODO: Convert to external model client\n",
        "  return mpve_domain_expert_model_based_eval(reference, prediction, client=None) # Client=None for external api\n",
        "\n",
        "\n",
        "# TODO: Convert to Drive path\n",
        "_EVAL_PROMPT_FILENAME = file_root_path + \"/prompts/mat_eval_output_1_shot.txt\"\n",
        "\n",
        "\n",
        "def parse_ground_truth_mpve(ground_truth: str) -> list[dict[str, Any]]:\n",
        "  json_ground_truth = json5.loads(ground_truth.replace(\"\\n\", \"\"))\n",
        "  json_ground_truth.sort(key=lambda x: x[\"index\"])\n",
        "  # remove unnecessary fields for model eval to prevent hallucination\n",
        "  for item in json_ground_truth:\n",
        "    if \"index\" in item:\n",
        "      del item[\"index\"]\n",
        "    if \"paper_id\" in item:\n",
        "      del item[\"paper_id\"]\n",
        "    if \"synonyms\" in item:\n",
        "      del item[\"synonyms\"]\n",
        "  return json_ground_truth\n",
        "\n",
        "\n",
        "def parse_model_response_mpve(model_response: str) -> list[dict[str, Any]]:\n",
        "  \"\"\"Parses model response.\n",
        "\n",
        "  Args:\n",
        "    model_response:\n",
        "\n",
        "  Returns:\n",
        "  \"\"\"\n",
        "  response_text = (\n",
        "      model_response.replace(\"\\n\", \"\")\n",
        "      .removeprefix(\" \")\n",
        "      .removesuffix(\" \")\n",
        "      .removeprefix(\"```json\")\n",
        "      .removesuffix(\"```json\")\n",
        "      .removeprefix(\"```\")\n",
        "      .removesuffix(\"```\")\n",
        "      .removeprefix(\"`\")\n",
        "      .removesuffix(\"`\")\n",
        "  )\n",
        "  try:\n",
        "    formatted_text = re.sub(r\"(?<=\\w)'(?=\\w|\\s)\", \"\\\\'\", response_text)\n",
        "    if not formatted_text:\n",
        "      formatted_text = \"{}\"\n",
        "      print(\"Response_text is empty\")\n",
        "    json_model_response = json5.loads(formatted_text)\n",
        "  except Exception as e:  # pylint: disable=broad-except\n",
        "    print(\"Skipping incomplete last item: \", e)\n",
        "    ind = [m.start() for m in re.finditer(r\",\\s*\\{\", response_text)][-1]\n",
        "    json_model_response = json5.loads(response_text[:ind] + \"]\")\n",
        "  return json_model_response\n",
        "\n",
        "\n",
        "def mpv_model_eval_paper(\n",
        "    record_id: str | None,\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    eval_prompt: str,\n",
        "    client: Any,\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Runs model evaluation on material properties for a single paper.\n",
        "\n",
        "  Args:\n",
        "    record_id: record id or paper id.\n",
        "    ground_truth: ground truth list in str type.\n",
        "    model_response: model response list in str type.\n",
        "    eval_prompt: eval prompt.\n",
        "    client: llm client.\n",
        "\n",
        "  Returns:\n",
        "    model eval response json.\n",
        "  \"\"\"\n",
        "  json_ground_truth = parse_ground_truth_mpve(ground_truth)\n",
        "  json_model_response = parse_model_response_mpve(model_response)\n",
        "  return model_eval_json(\n",
        "      record_id=record_id,\n",
        "      json_ground_truth=json_ground_truth,\n",
        "      json_model_response=json_model_response,\n",
        "      eval_prompt=eval_prompt,\n",
        "      client=client,\n",
        "  )\n",
        "\n",
        "\n",
        "def filter_ground_truth_properties(ground_truth: str) -> list[dict[str, Any]]:\n",
        "  \"\"\"Filters ground truth to only keep the properties we want to evaluate.\n",
        "\n",
        "  Args:\n",
        "    ground_truth: ground truth list in str type.\n",
        "\n",
        "  Returns:\n",
        "    filtered ground truth list.\n",
        "  \"\"\"\n",
        "  json_ground_truth = json5.loads(ground_truth)\n",
        "  filtered_ground_truth = []\n",
        "  valid_property_names = [\n",
        "      \"bandgap\",\n",
        "      \"band gap\",\n",
        "      \"gap energy\",\n",
        "      \"energy gap\",\n",
        "      \"refractive_index\",\n",
        "      \"refractive index\",\n",
        "      \"index of refraction\",\n",
        "      \"n-value\",\n",
        "      \"n value\",\n",
        "  ]\n",
        "  for item in json_ground_truth:\n",
        "    for valid_property_name in valid_property_names:\n",
        "      if valid_property_name in item[\"property_name\"].lower():\n",
        "        filtered_ground_truth.append(item)\n",
        "        break\n",
        "  return filtered_ground_truth\n",
        "\n",
        "\n",
        "def mpve_domain_expert_model_based_eval(\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    client: Any | None = None,\n",
        "    eval_prompt: str = _EVAL_PROMPT_FILENAME,\n",
        "    verbose: bool = True,\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Runs model based eval on material properties.\n",
        "\n",
        "  Args:\n",
        "    ground_truth: ground truth list in str type.\n",
        "    model_response: model response list in str type.\n",
        "    client: llm client.\n",
        "    eval_prompt: eval prompt.\n",
        "    verbose: whether to print out eval results.\n",
        "\n",
        "  Returns:\n",
        "    eval result.\n",
        "  \"\"\"\n",
        "  if verbose:\n",
        "    print(\"Model eval started...\")\n",
        "  eval_output_item = mpv_model_eval_paper(\n",
        "      record_id=None,\n",
        "      ground_truth=ground_truth,\n",
        "      model_response=model_response,\n",
        "      eval_prompt=eval_prompt,\n",
        "      client=client,\n",
        "  )\n",
        "  if verbose:\n",
        "    print(\"Model eval finished.\")\n",
        "  eval_result = eval_overall_result(\n",
        "      eval_output_item, verbose=verbose\n",
        "  )\n",
        "  if verbose:\n",
        "    print(\"Eval results:\\n\", eval_result)\n",
        "  return eval_result\n",
        "\n",
        "def load_matsci_prompt(filepath: str) -> str:\n",
        "  \"\"\"Loads matsci prompt.\n",
        "\n",
        "  Args:\n",
        "    filepath: filepath of prompt.\n",
        "\n",
        "  Returns:\n",
        "    Loaded prompt.\n",
        "  \"\"\"\n",
        "  # return resources.GetResource(filepath).decode(\"utf-8\").strip()\n",
        "  with open(filepath, 'r') as file:\n",
        "    text_content = file.read()\n",
        "  return text_content.strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval_overall_result(\n",
        "    eval_output_item: dict[str, Any], verbose: bool = False\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Gets overall eval result.\n",
        "\n",
        "  Args:\n",
        "    eval_output_item: eval output item.\n",
        "    verbose: whether to print model eval original output.\n",
        "\n",
        "  Returns:\n",
        "    overall eval result.\n",
        "  \"\"\"\n",
        "  num_match = sum([\n",
        "      1 if (\"json_extracted_index\" in item) else 0\n",
        "      for item in eval_output_item[\"response_json\"]\n",
        "  ])\n",
        "  if verbose:\n",
        "    print(\"Model eval original output:\\n\", eval_output_item)\n",
        "  num_gt = eval_output_item[\"ground_truth_length\"]\n",
        "  num_response = eval_output_item[\"model_response_length\"]\n",
        "  pre = min(num_match / num_response if num_response else np.nan, 1.0)\n",
        "  rec = min(num_match / num_gt if num_gt else np.nan, 1.0)\n",
        "  return {\n",
        "      \"num_match\": num_match,\n",
        "      \"num_ground_truth\": num_gt,\n",
        "      \"num_model_response\": num_response,\n",
        "      \"precision\": pre,\n",
        "      \"recall\": rec,\n",
        "      \"f1\": 2.0 * pre * rec / (pre + rec) if pre + rec else 0.0,\n",
        "  }"
      ],
      "metadata": {
        "id": "z6ufJ9PGHxo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bert eval"
      ],
      "metadata": {
        "id": "NwlQaQlVmRcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_score(prediction, reference):\n",
        "  precision, recall, F1 = score([prediction], [reference], lang=\"en\", verbose=False)\n",
        "  return {\"bert_precision\": precision.item(), \"bert_recall\" : recall.item(), \"bert_f1\": F1.item()}"
      ],
      "metadata": {
        "id": "cXZW-yhWRkLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## rouge eval"
      ],
      "metadata": {
        "id": "gHb00g_6mTRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import six\n",
        "import collections\n",
        "\n",
        "class AggregateScore(\n",
        "    collections.namedtuple(\"AggregateScore\", [\"low\", \"mid\", \"high\"])):\n",
        "  \"\"\"Tuple containing confidence intervals for scores.\"\"\"\n",
        "\n",
        "class BootstrapAggregator(object):\n",
        "  \"\"\"Aggregates scores to provide confidence intervals.\n",
        "\n",
        "  Sample usage:\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'])\n",
        "    aggregator = Aggregator()\n",
        "    aggregator.add_scores(scorer.score(\"one two three\", \"one two\"))\n",
        "    aggregator.add_scores(scorer.score(\"one two five six\", \"seven eight\"))\n",
        "    result = aggregator.aggregate()\n",
        "    print result\n",
        "    {'rougeL': AggregateScore(\n",
        "         low=Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
        "         mid=Score(precision=0.5, recall=0.33, fmeasure=0.40),\n",
        "         high=Score(precision=1.0, recall=0.66, fmeasure=0.80)),\n",
        "     'rouge1': AggregateScore(\n",
        "         low=Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
        "         mid=Score(precision=0.5, recall=0.33, fmeasure=0.40),\n",
        "         high=Score(precision=1.0, recall=0.66, fmeasure=0.80))}\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, confidence_interval=0.95, n_samples=1000):\n",
        "    \"\"\"Initializes a BootstrapAggregator object.\n",
        "\n",
        "    Args:\n",
        "      confidence_interval: Confidence interval to compute on the mean as a\n",
        "        decimal.\n",
        "      n_samples: Number of samples to use for bootstrap resampling.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If invalid argument is given.\n",
        "    \"\"\"\n",
        "\n",
        "    if confidence_interval < 0 or confidence_interval > 1:\n",
        "      raise ValueError(\"confidence_interval must be in range [0, 1]\")\n",
        "    if n_samples <= 0:\n",
        "      raise ValueError(\"n_samples must be positive\")\n",
        "\n",
        "    self._n_samples = n_samples\n",
        "    self._confidence_interval = confidence_interval\n",
        "    self._scores = collections.defaultdict(list)\n",
        "\n",
        "  def add_scores(self, scores):\n",
        "    \"\"\"Adds a sample for future aggregation.\n",
        "\n",
        "    Args:\n",
        "      scores: Dict mapping score_type strings to a namedtuple object/class\n",
        "        representing a score.\n",
        "    \"\"\"\n",
        "\n",
        "    for score_type, score in six.iteritems(scores):\n",
        "      self._scores[score_type].append(score)\n",
        "\n",
        "  def aggregate(self):\n",
        "    \"\"\"Aggregates scores previously added using add_scores.\n",
        "\n",
        "    Returns:\n",
        "      A dict mapping score_type to AggregateScore objects.\n",
        "    \"\"\"\n",
        "\n",
        "    result = {}\n",
        "    for score_type, scores in six.iteritems(self._scores):\n",
        "      # Stack scores into a 2-d matrix of (sample, measure).\n",
        "      score_matrix = np.vstack(tuple(scores))\n",
        "      # Percentiles are returned as (interval, measure).\n",
        "      percentiles = self._bootstrap_resample(score_matrix)\n",
        "      # Extract the three intervals (low, mid, high).\n",
        "      intervals = tuple(\n",
        "          (scores[0].__class__(*percentiles[j, :]) for j in range(3)))\n",
        "      result[score_type] = AggregateScore(\n",
        "          low=intervals[0], mid=intervals[1], high=intervals[2])\n",
        "    return result\n",
        "\n",
        "  def _bootstrap_resample(self, matrix):\n",
        "    \"\"\"Performs bootstrap resampling on a matrix of scores.\n",
        "\n",
        "    Args:\n",
        "      matrix: A 2-d matrix of (sample, measure).\n",
        "\n",
        "    Returns:\n",
        "      A 2-d matrix of (bounds, measure). There are three bounds: low (row 0),\n",
        "      mid (row 1) and high (row 2). Mid is always the mean, while low and high\n",
        "      bounds are specified by self._confidence_interval (which defaults to 0.95\n",
        "      meaning it will return the 2.5th and 97.5th percentiles for a 95%\n",
        "      confidence interval on the mean).\n",
        "    \"\"\"\n",
        "\n",
        "    # Matrix of (bootstrap sample, measure).\n",
        "    sample_mean = np.zeros((self._n_samples, matrix.shape[1]))\n",
        "    for i in range(self._n_samples):\n",
        "      sample_idx = np.random.choice(\n",
        "          np.arange(matrix.shape[0]), size=matrix.shape[0])\n",
        "      sample = matrix[sample_idx, :]\n",
        "      sample_mean[i, :] = np.mean(sample, axis=0)\n",
        "\n",
        "    # Take percentiles on the estimate of the mean using bootstrap samples.\n",
        "    # Final result is a (bounds, measure) matrix.\n",
        "    percentile_delta = (1 - self._confidence_interval) / 2\n",
        "    q = 100 * np.array([percentile_delta, 0.5, 1 - percentile_delta])\n",
        "    return np.percentile(sample_mean, q, axis=0)\n",
        "\n",
        "def _prepare_summary_rouge(summary):\n",
        "  # Make sure the summary is not bytes-type\n",
        "  # Add newlines between sentences so that rougeLsum is computed correctly.\n",
        "  summary = summary.replace(\" . \", \" .\\n\")\n",
        "  return summary\n",
        "\n",
        "def get_rouge_score(prediction, reference):\n",
        "  score_keys = ['rouge1', 'rouge2', 'rougeLsum']\n",
        "  predictions = [prediction]\n",
        "  targets = [reference]\n",
        "  scorer = rouge_scorer.RougeScorer(score_keys)\n",
        "  count = 0\n",
        "  sum_scores = collections.defaultdict(float)\n",
        "  for prediction, target in zip(predictions, targets):\n",
        "    target = _prepare_summary_rouge(target)\n",
        "    prediction = _prepare_summary_rouge(prediction)\n",
        "    scores = scorer.score(target=target, prediction=prediction)\n",
        "    count += 1\n",
        "    for k, v in scores.items():\n",
        "      sum_scores[k] += v.fmeasure\n",
        "  if count == 0:\n",
        "    raise ValueError(\"Predictions and targets must both have nonzero length\")\n",
        "  result = {k: v / count for k, v in sum_scores.items()}\n",
        "  return {key: result[key] * 100 for key in score_keys}"
      ],
      "metadata": {
        "id": "4vaHHyi3VIyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## biogr eval"
      ],
      "metadata": {
        "id": "7zqg5F8Ll9BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def center_of_bbox(json_coords: dict[str, float]) -> dict[str, float]:\n",
        "  # NOTE: This doesn't work if you wrap around the 180 --> -179\n",
        "  # longitude line in the Pacific but we don't have it in our data.\n",
        "  bbox_center = {}\n",
        "  bbox_center[\"lat\"] = np.mean([json_coords[\"S\"], json_coords[\"N\"]])\n",
        "  bbox_center[\"lng\"] = np.mean([json_coords[\"E\"], json_coords[\"W\"]])\n",
        "  return bbox_center\n",
        "\n",
        "\n",
        "def compute_distance(\n",
        "    lat1_deg: float, lng1_deg: float, lat2_deg: float, lng2_deg: float\n",
        ") -> float:\n",
        "  \"\"\"Computes the distance between two points on a sphere in meters.\n",
        "\n",
        "  Args:\n",
        "    lat1_deg: Latitude of the first point in degrees.\n",
        "    lng1_deg: Longitude of the first point in degrees.\n",
        "    lat2_deg: Latitude of the second point in degrees.\n",
        "    lng2_deg: Longitude of the second point in degrees.\n",
        "\n",
        "  Returns:\n",
        "    The distance between the two points in meters.\n",
        "  \"\"\"\n",
        "  # Haversine Formula for the geodesic distance on a sphere.\n",
        "  # See https://en.wikipedia.org/wiki/Haversine_formula.\n",
        "  lat1 = np.deg2rad(lat1_deg)\n",
        "  lng1 = np.deg2rad(lng1_deg)\n",
        "  lat2 = np.deg2rad(lat2_deg)\n",
        "  lng2 = np.deg2rad(lng2_deg)\n",
        "\n",
        "  alpha = np.sin((lat2 - lat1) * 0.5)\n",
        "  gamma = np.sin((lng2 - lng1) * 0.5)\n",
        "  alpha = alpha * alpha + np.cos(lat1) * np.cos(lat2) * gamma * gamma\n",
        "  if alpha > 1.0:\n",
        "    alpha = 1.0  # bulletproof sqrt(1-alpha)\n",
        "  gamma = 2.0 * np.arctan2(np.sqrt(alpha), np.sqrt(1.0 - alpha))\n",
        "  return 6371000 * gamma\n",
        "\n",
        "\n",
        "def compute_center_error_km(\n",
        "    center_prediction: dict[str, float], center_ground_truth: dict[str, float]\n",
        ") -> float:\n",
        "  return 0.001 * compute_distance(\n",
        "      center_prediction[\"lat\"],\n",
        "      center_prediction[\"lng\"],\n",
        "      center_ground_truth[\"lat\"],\n",
        "      center_ground_truth[\"lng\"],\n",
        "  )\n",
        "\n",
        "\n",
        "def compute_box_size_km(coords: dict[str, float]) -> float:\n",
        "  # Returns half of the diagonal (e.g. like half of a TV). This corresponds\n",
        "  # to the radius of a circle that inscribes the rectangle.\n",
        "  return (\n",
        "      0.001\n",
        "      * 0.5\n",
        "      * compute_distance(coords[\"S\"], coords[\"W\"], coords[\"N\"], coords[\"E\"])\n",
        "  )\n",
        "\n",
        "\n",
        "def compute_distance_metrics(\n",
        "    prediction: dict[str, float], ground_truth: dict[str, float]\n",
        ") -> dict[str, float]:\n",
        "  \"\"\"Computes distance metrics between the prediction and ground truth.\n",
        "\n",
        "  Computes two distance metrics:\n",
        "  normalized_distance_error - Distance between the center of the predicted\n",
        "  bounding box and the center of the ground truth bounding box, normalized by\n",
        "  the ground truth box radius.\n",
        "  relative_box_size - Ratio of predicted box size to ground truth box size\n",
        "  (using the diagonal length as the size metric).\n",
        "\n",
        "  Args:\n",
        "    prediction: A dictionary with the prediction coordinates.\n",
        "    ground_truth: A dictionary with the ground truth coordinates.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary with the normalized distance error between the predicted and\n",
        "    ground truth box centers and the relative size of the predicted box.\n",
        "  \"\"\"\n",
        "\n",
        "  center_ground_truth = center_of_bbox(ground_truth)\n",
        "  center_prediction = center_of_bbox(prediction)\n",
        "  center_error_km = compute_center_error_km(\n",
        "      center_prediction, center_ground_truth\n",
        "  )\n",
        "\n",
        "  ground_truth_box_size_km = compute_box_size_km(ground_truth)\n",
        "  normalized_distance_error = center_error_km / ground_truth_box_size_km\n",
        "\n",
        "  prediction_box_size_km = compute_box_size_km(prediction)\n",
        "  relative_size = prediction_box_size_km / ground_truth_box_size_km\n",
        "\n",
        "  return {\n",
        "      \"normalized_distance_error\": normalized_distance_error,\n",
        "      \"relative_box_size\": relative_size,\n",
        "  }\n",
        "\n",
        "def coords_to_box(coords: dict[str, float]) -> np.ndarray:\n",
        "  return np.array([coords[\"W\"], coords[\"S\"], coords[\"E\"], coords[\"N\"]])\n",
        "\n",
        "\n",
        "def parse_biodiversity_response(model_response: str) -> dict[str, float]:\n",
        "  \"\"\"Parses a model response string into a dictionary of coordinates.\n",
        "\n",
        "  Args:\n",
        "    model_response: The model response string to be parsed.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing the W, E, S, N values.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If the model response string cannot be parsed into either of the\n",
        "      supported formats.\n",
        "  \"\"\"\n",
        "  if \"{\" in model_response and \"}\" in model_response:\n",
        "    cleaned_response = model_response[\n",
        "        model_response.find(\"{\") : model_response.rfind(\"}\") + 1\n",
        "    ]\n",
        "    return json5.loads(cleaned_response)\n",
        "  if all(key in model_response for key in [\"W\", \"E\", \"S\", \"N\"]):\n",
        "    return {\n",
        "        \"W\": float(model_response.split('\"W\":')[-1].split(\",\")[0].split()[0]),\n",
        "        \"E\": float(model_response.split('\"E\":')[-1].split(\",\")[0].split()[0]),\n",
        "        \"S\": float(model_response.split('\"S\":')[-1].split(\",\")[0].split()[0]),\n",
        "        \"N\": float(model_response.split('\"N\":')[-1].split(\",\")[0].split()[0]),\n",
        "    }\n",
        "  raise ValueError(\"Can not parse model response\")\n",
        "\n",
        "\n",
        "def bb_intersection_over_union(box_a: np.ndarray, box_b: np.ndarray) -> float:\n",
        "  \"\"\"Calculates the Intersection over Union (IoU) between two bounding boxes.\n",
        "\n",
        "  Args:\n",
        "    box_a: A list of coordinates representing the first bounding box.\n",
        "    box_b: A list of coordinates representing the second bounding box.\n",
        "\n",
        "  Returns:\n",
        "    The IoU value, a float between 0 and 1.\n",
        "    0 indicates no overlap and 1 indicates perfect overlap.\n",
        "  \"\"\"\n",
        "\n",
        "  def _intersection_area(box_a: np.ndarray, box_b: np.ndarray) -> float:\n",
        "    x_a = max(box_a[0], box_b[0])\n",
        "    y_a = max(box_a[1], box_b[1])\n",
        "    x_b = min(box_a[2], box_b[2])\n",
        "    y_b = min(box_a[3], box_b[3])\n",
        "\n",
        "    width = x_b - x_a\n",
        "    height = y_b - y_a\n",
        "    if (width < 0) or (height < 0):\n",
        "      return 0.0\n",
        "    return width * height\n",
        "\n",
        "  def _area(box: np.ndarray) -> float:\n",
        "    return (box[2] - box[0]) * (box[3] - box[1])\n",
        "\n",
        "  inter_area = _intersection_area(box_a, box_b)\n",
        "  union_area = _area(box_a) + _area(box_b) - inter_area\n",
        "  return inter_area / float(union_area)\n",
        "\n",
        "\n",
        "def biodiversity_georeferencing_eval(\n",
        "    model_response: str, ground_truth: str, verbosity: int = 0\n",
        ") -> dict[str, Union[float, str]]:\n",
        "  \"\"\"Computes IOU between ground truth and model response bounding boxes.\n",
        "\n",
        "  Args:\n",
        "    ground_truth: A JSON string with lat/lng bounding box coordinates\n",
        "    model_response: A JSON string with the model response.\n",
        "    verbosity: Used for debugging.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary with IOU keyed as \"iou\".\n",
        "  \"\"\"\n",
        "  # Load in ground truth coordinates.\n",
        "  ground_truth_coords = json5.loads(ground_truth)\n",
        "\n",
        "  try:\n",
        "    predicted_coords = parse_biodiversity_response(model_response)\n",
        "  except Exception:  # pylint: disable=broad-except\n",
        "    if verbosity > 0:\n",
        "      print(\"Failed to extract coords from model response: \", model_response)\n",
        "    return {\"iou\": \"Can not parse model response\"}\n",
        "\n",
        "  iou = bb_intersection_over_union(\n",
        "      coords_to_box(ground_truth_coords), coords_to_box(predicted_coords)\n",
        "  )\n",
        "\n",
        "  distance_metrics = compute_distance_metrics(\n",
        "      predicted_coords, ground_truth_coords\n",
        "  )\n",
        "  biogr_metrics = {\n",
        "      \"iou\": iou,\n",
        "      \"normalized_distance_error\": distance_metrics[\n",
        "          \"normalized_distance_error\"\n",
        "      ],\n",
        "      \"relative_box_size\": distance_metrics[\"relative_box_size\"],\n",
        "  }\n",
        "  return biogr_metrics"
      ],
      "metadata": {
        "id": "9m4pj32Pi2y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pdb eval"
      ],
      "metadata": {
        "id": "SBaTJhefmM8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_sequence_alignment_counts(\n",
        "    sequence_1: str, sequence_2: str\n",
        ") -> Dict[str, Union[str, int]]:\n",
        "  \"\"\"Calculates the counts of gaps, identities, and mismatches in the best alignment of two sequences.\n",
        "\n",
        "  Args:\n",
        "    sequence_1: The first sequence to be aligned.\n",
        "    sequence_2: The second sequence to be aligned.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing the counts of gaps, identities, and mismatches:\n",
        "      - \"n_gaps\": Number of gap characters introduced in the alignment.\n",
        "      - \"n_identities\": Number of positions where the characters are identical.\n",
        "      - \"n_mismatches\": Number of positions where the characters are different.\n",
        "      - \"normalized_levenshtein_distance\": Levenshtein distance divided by the\n",
        "          length of the longer sequence. Levenshtein distance is the minimum\n",
        "          number of edits needed to transform one sequence into another.\n",
        "      - \"identity_ratio\": n_identities divided by the length of the alignment.\n",
        "  \"\"\"\n",
        "  # Create an alignment object.\n",
        "  sequence_1 = sequence_1 if sequence_1 else \" \"\n",
        "  sequence_2 = sequence_2 if sequence_2 else \" \"\n",
        "  aligner = Align.PairwiseAligner()\n",
        "  best_alignment = aligner.align(sequence_1, sequence_2)[0]\n",
        "\n",
        "  max_length = max(len(sequence_1), len(sequence_2))\n",
        "  if max_length == 0:\n",
        "    normalized_distance = \"Zero length sequences\"\n",
        "  else:\n",
        "    normalized_distance = (\n",
        "        Levenshtein.distance(sequence_1, sequence_2) / max_length\n",
        "    )\n",
        "  if not best_alignment[0]:\n",
        "    identity_ratio = \"Zero length alignment\"\n",
        "  else:\n",
        "    identity_ratio = best_alignment.counts().identities / len(best_alignment[0])\n",
        "\n",
        "  return {\n",
        "      \"n_gaps\": best_alignment.counts().gaps,\n",
        "      \"n_identities\": best_alignment.counts().identities,\n",
        "      \"n_mismatches\": best_alignment.counts().mismatches,\n",
        "      \"normalized_levenshtein_distance\": normalized_distance,\n",
        "      \"identity_ratio\": identity_ratio,\n",
        "  }\n",
        "\n",
        "def extract_code_block(response_str: str) -> tuple[str, str]:\n",
        "  \"\"\"Extract code block and function name from response string.\n",
        "\n",
        "  Args:\n",
        "    response_str: The model response string.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the code block and function name.\n",
        "  \"\"\"\n",
        "  # We assume that the code block is always in a ```python block. Other language\n",
        "  # blocks are not supported yet.\n",
        "  if \"```python\" in response_str:\n",
        "    start_idx = response_str.index(\"```python\")\n",
        "    try:\n",
        "      end_idx = response_str.index(\"```\", start_idx + 1)\n",
        "    except ValueError:\n",
        "      # Some times the model tries to add the input string which will exceed\n",
        "      # the length of the decoded response. In this case, we will just return\n",
        "      # the code block and function name.\n",
        "      # Find where return is called and get the end of the line\n",
        "      return_idx = response_str.index(\"return\")\n",
        "      end_idx = response_str.index(\"\\n\", return_idx)\n",
        "    code_block = response_str[start_idx + 9 : end_idx]\n",
        "    function_name = response_str.split(\"def \")[1].split(\"(\")[0]\n",
        "    return code_block, function_name\n",
        "  else:\n",
        "    return \"\", \"\"\n",
        "\n",
        "def pdb_execute_code_eval(\n",
        "    model_response: str,\n",
        "    input_data_prompt: str,\n",
        ") -> str:\n",
        "  \"\"\"Executes the code block and returns the pdb eval.\n",
        "\n",
        "  Args:\n",
        "    model_response: A string containing the model's response, also expected to\n",
        "      have the predicted protein sequence on the line following a '>' line.\n",
        "    input_data_prompt: The prompt text including the input data used for running\n",
        "      inference on the model. This is used to reconstruct the input pdb string\n",
        "      to the model and pass it to the code block generated in the model\n",
        "      response.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing the pdb eval metrics.\n",
        "  \"\"\"\n",
        "  code_block, function_name = extract_code_block(model_response)\n",
        "  pred_output = \"\"\n",
        "  if not code_block and not function_name:\n",
        "    return \"\"\n",
        "  try:\n",
        "    local_namespace = {}\n",
        "    exec(code_block, {}, local_namespace)  # pylint: disable=exec-used\n",
        "    pdb_data_string = \"ATOM\" + input_data_prompt.split(\"ATOM\", 1)[1]\n",
        "    if function_name in local_namespace:\n",
        "      pred_output = local_namespace[function_name](pdb_data_string)\n",
        "      return pred_output\n",
        "    else:\n",
        "      print(\"Function name not found in local namespace: \", function_name)\n",
        "  except SyntaxError as e:\n",
        "    print(\"SyntaxError: \", e)\n",
        "\n",
        "  return pred_output\n",
        "\n",
        "def pdb_reconstruction_eval(\n",
        "    model_response: str,\n",
        "    ground_truth_json_str: str,\n",
        "    input_data_prompt: str,\n",
        ") -> dict[str, Union[str, float]]:\n",
        "  \"\"\"Evaluates the alignment of a predicted sequence.\n",
        "\n",
        "  Args:\n",
        "    ground_truth_json_str: A json string containing the ground truth protein\n",
        "      sequence data.\n",
        "    model_response: A string containing the model's response, also expected to\n",
        "      have the predicted protein sequence on the line following a '>' line.\n",
        "    input_data_prompt: The prompt text including the input data used for running\n",
        "      inference on the model. This is used to reconstruct the input pdb string\n",
        "      to the model and pass it to the code block generated in the model\n",
        "      response.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing sequence alignment metrics.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    targets = json5.loads(ground_truth_json_str)[\"sequence\"]\n",
        "  except ValueError:\n",
        "    return {\n",
        "        \"n_gaps\": \"Can not parse ground truth\",\n",
        "        \"n_identities\": \"Can not parse ground truth\",\n",
        "        \"n_mismatches\": \"Can not parse ground truth\",\n",
        "        \"normalized_levenshtein_distance\": \"Can not parse ground truth\",\n",
        "        \"identity_ratio\": \"Can not parse ground truth\",\n",
        "        \"rouge1\": \"Can not parse ground truth\",\n",
        "        \"rouge2\": \"Can not parse ground truth\",\n",
        "        \"rougeLsum\": \"Can not parse ground truth\",\n",
        "    }\n",
        "  if \"```python\" in model_response:\n",
        "    model_response = pdb_execute_code_eval(\n",
        "        model_response=model_response, input_data_prompt=input_data_prompt\n",
        "    )\n",
        "  model_response = model_response.replace(\"\\u003E\", \">\")\n",
        "  lines = model_response.splitlines()\n",
        "  for i, line in enumerate(lines):\n",
        "    if line.startswith(\">\") and i < len(lines) - 1:\n",
        "      parsed_model_response = lines[i + 1]\n",
        "      pdb_eval_results = best_sequence_alignment_counts(\n",
        "          targets, parsed_model_response\n",
        "      )\n",
        "\n",
        "      return pdb_eval_results\n",
        "  return {\n",
        "      \"n_gaps\": \"Can not parse model response\",\n",
        "      \"n_identities\": \"Can not parse model response\",\n",
        "      \"n_mismatches\": \"Can not parse model response\",\n",
        "      \"normalized_levenshtein_distance\": \"Can not parse model response\",\n",
        "      \"identity_ratio\": \"Can not parse model response\",\n",
        "      \"rouge1\": \"Can not parse model response\",\n",
        "      \"rouge2\": \"Can not parse model response\",\n",
        "      \"rougeLsum\": \"Can not parse model response\",\n",
        "  }\n"
      ],
      "metadata": {
        "id": "4A4r9tDe-itO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# eval helper functions"
      ],
      "metadata": {
        "id": "lsiSmrqpPslm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import raiseExceptions\n",
        "def get_annotated_structure_metadata_and_dft_params(\n",
        "    gt_paper_code: str, verbose: int = 0\n",
        ") -> dict[str, list[str]]:\n",
        "  \"\"\"Returns structure metadata and dft params from the ground truth code.\n",
        "\n",
        "  Args:\n",
        "    gt_paper_code: The ground truth code from .py file as a string.\n",
        "    verbose: The verbosity level.\n",
        "  \"\"\"\n",
        "  structures = []\n",
        "  dft_params = []\n",
        "\n",
        "  paper = gt_paper_code\n",
        "  if \"structure_metadata_\" in paper:\n",
        "    parts = paper.split(\"structure_metadata_\")[1:]\n",
        "    whole_parts = [\"structure_metadata_\" + part for part in parts]\n",
        "\n",
        "    for part in whole_parts:\n",
        "      if verbose > 1:\n",
        "        print(\"PART:\\n\", part)\n",
        "      if \"parse_raw(\" not in part:\n",
        "        continue\n",
        "      left, right, *_ = part.split(\"parse_raw(\")\n",
        "      if \"StructureMetadata\" in left:\n",
        "        end_struc = \")\"\n",
        "        if \"')\" in right:\n",
        "          end_struc = \"')\"\n",
        "        elif \"'\\n)\" in right:\n",
        "          end_struc = \"'\\n)\"\n",
        "        struc_json = right.split(end_struc)[0].strip()\n",
        "        if verbose > 0:\n",
        "          print(\"Extracted structure:\\n\", struc_json)\n",
        "        # clean_json = struc_json.replace('NaN', '\"NaN\"')\n",
        "        # struc_json = ast.literal_eval(clean_json)\n",
        "        structures.append(struc_json)\n",
        "\n",
        "  if \"dft_params_\" in paper:\n",
        "    parts = paper.split(\"dft_params_\")[1:]\n",
        "    # print(parts)\n",
        "    whole_parts = [\"dft_params_\" + part for part in parts]\n",
        "\n",
        "    for part in whole_parts:\n",
        "      if verbose > 1:\n",
        "        print(\"PART:\\n\", part)\n",
        "      if \"parse_raw(\" not in part:\n",
        "        continue\n",
        "      left, right, *_ = part.split(\"parse_raw(\")\n",
        "      if \"DFTParameters\" in left:\n",
        "        end_struc = \")\"\n",
        "        if \"')\" in right:\n",
        "          end_struc = \"')\"\n",
        "        elif \"'\\n)\" in right:\n",
        "          end_struc = \"'\\n)\"\n",
        "        dft_params_str = right.split(end_struc)[0].strip()\n",
        "        if verbose > 0:\n",
        "          print(\"Extracted dft_param:\\n\", dft_params_str)\n",
        "        # clean_json = dft_params_str.replace('NaN', '\"NaN\"')\n",
        "        # dft_params_str = ast.literal_eval(clean_json)\n",
        "        dft_params.append(dft_params_str)\n",
        "\n",
        "  gt_struc_jsons = []\n",
        "  for struct_metadata in structures:\n",
        "    gt_json = struct_metadata.split(\"'\")[1]\n",
        "    clean_json = gt_json.replace(\"NaN\", '\"NaN\"')\n",
        "    try:\n",
        "      gt_structure_json = ast.literal_eval(clean_json)\n",
        "      gt_struc_jsons.append(gt_structure_json)\n",
        "    except Exception:  # pylint: disable=broad-exception-caught\n",
        "      gt_struc_jsons.append(clean_json)\n",
        "\n",
        "  gt_dft_params_jsons = []\n",
        "  for dft_param in dft_params:\n",
        "    gt_json = dft_param.split(\"'\")[1]\n",
        "    clean_json = gt_json.replace(\"NaN\", '\"NaN\"')\n",
        "    try:\n",
        "      gt_dft_json = ast.literal_eval(clean_json)\n",
        "      gt_dft_params_jsons.append(gt_dft_json)\n",
        "    except Exception:  # pylint: disable=broad-exception-caught\n",
        "      gt_dft_params_jsons.append(clean_json)\n",
        "\n",
        "  return {\n",
        "      \"structures_metadata\": gt_struc_jsons,\n",
        "      \"dft_params\": gt_dft_params_jsons,\n",
        "  }\n",
        "\n",
        "\n",
        "def preprocess_ground_truth(\n",
        "    ground_truth: str, task_name: str, prompt: str\n",
        ") -> str:\n",
        "  \"\"\"Preprocesses the ground truth before sending to eval.\"\"\"\n",
        "  # Drops the record_ids.\n",
        "  json_gt = json5.loads(ground_truth)\n",
        "  if isinstance(json_gt, dict):\n",
        "    json_gt.pop(\"record_id\", None)\n",
        "    json_gt.pop(\"arxiv_id\", None)\n",
        "    json_gt.pop(\"paper_id\", None)\n",
        "  if isinstance(json_gt, list):\n",
        "    for item in json_gt:\n",
        "      if isinstance(item, dict):\n",
        "        item.pop(\"record_id\", None)\n",
        "        item.pop(\"arxiv_id\", None)\n",
        "        item.pop(\"paper_id\", None)\n",
        "  groundtruth_with_no_ids = json5.dumps(json_gt)\n",
        "  # Preprocess for dft metadata tasks.\n",
        "  if task_name == \"dft\" and prompt == \"extract_dft_metadata_1_shot\":\n",
        "    processed = get_annotated_structure_metadata_and_dft_params(\n",
        "        groundtruth_with_no_ids\n",
        "    )[\"dft_params\"]\n",
        "  elif task_name == \"dft\" and prompt == \"extract_structure_data_1_shot\":\n",
        "    processed = get_annotated_structure_metadata_and_dft_params(\n",
        "        groundtruth_with_no_ids\n",
        "    )[\"structures_metadata\"]\n",
        "  else:\n",
        "    processed = groundtruth_with_no_ids\n",
        "  return str(processed)\n",
        "\n",
        "\n",
        "def read_task_ground_truth_and_response(\n",
        "    ground_truth_path: str,\n",
        "    model_response_path: str,\n",
        ") -> Tuple[str, str, str]:\n",
        "  \"\"\"Reads in the ground truth and response for all tasks.\"\"\"\n",
        "  try:\n",
        "    # Gets the ground truth.\n",
        "    with open(ground_truth_path, \"r\") as f:\n",
        "      ground_truth_info = f.read()\n",
        "\n",
        "    model_response = \"\"\n",
        "    inf_prompt = \"\"\n",
        "    if os.path.exists(model_response_path):\n",
        "      with open(model_response_path, \"r\") as f:\n",
        "        full_model_response = json5.loads(f.read())\n",
        "        if \"response_text\" in full_model_response:\n",
        "          model_response = full_model_response[\"response_text\"]\n",
        "        else:\n",
        "          raise ValueError(\n",
        "              f\"ERROR: The succeeded response for {model_response_path} does not contain response_text.\"\n",
        "          )\n",
        "        if 'pdb' in ground_truth_path:\n",
        "          if 'prompt_text' in full_model_response:\n",
        "            inf_prompt = full_model_response[\"prompt_text\"]\n",
        "          else:\n",
        "            raise ValueError(\n",
        "                f\"ERROR: The succeeded response for {model_response_path} does not contain prompt_text.\"\n",
        "            )\n",
        "\n",
        "    failed_model_response = model_response_path.replace(\"success\", \"failure\")\n",
        "    # Gets the response.\n",
        "    exception_message = \"\"\n",
        "    if os.path.exists(failed_model_response):\n",
        "      with open(failed_model_response, \"r\") as f:\n",
        "        full_model_response = json5.loads(f.read())\n",
        "        if \"exception_message\" in full_model_response:\n",
        "          exception_message = full_model_response[\"exception_message\"]\n",
        "        elif \"command-r-plus\" in failed_model_response and \"response_text\" in full_model_response:\n",
        "          exception_message = full_model_response[\"response_text\"]\n",
        "        else:\n",
        "          raise ValueError(\n",
        "              f\"ERROR: The failure response for {failed_model_response} does not contain exception message.\"\n",
        "          )\n",
        "\n",
        "    return ground_truth_info, model_response, exception_message, inf_prompt\n",
        "  except Exception as e:\n",
        "    print(f\"ERROR: {e}\")\n",
        "    raise Exception(e)"
      ],
      "metadata": {
        "id": "Kdc30iBVLY9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## static configs"
      ],
      "metadata": {
        "id": "brIrWo-gPzB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_SHARED_METRCS = [get_rouge_score, get_bert_score]\n",
        "_FULL_ADDITIONAL_METRICS = {\n",
        "    \"pdb\": {\n",
        "        \"reconstruct_protein_amino_acid_sequence_0_shot\": {\n",
        "            pdb_reconstruction_eval\n",
        "        },\n",
        "    },\n",
        "    \"mpve\": {\n",
        "        \"mat_paper_to_property_1_shot\": {\n",
        "            get_lmsim_score_mpve\n",
        "        },\n",
        "        \"mat_paper_to_property_1_shot_exclude_trivia\": {\n",
        "            get_lmsim_score_mpve\n",
        "        },\n",
        "        \"mat_paper_to_property_1_shot_bandgap_refractive\": {\n",
        "            get_lmsim_score_mpve\n",
        "        }\n",
        "    },\n",
        "    \"dft\": {\n",
        "        \"extract_structure_data_1_shot\": {\n",
        "            get_lmsim_score_dft\n",
        "        },\n",
        "        \"extract_dft_metadata_1_shot\": {\n",
        "            get_lmsim_score_dft\n",
        "        },\n",
        "    },\n",
        "    \"biogr\": {\n",
        "        \"georeference_image_0_shot\": {\n",
        "            biodiversity_georeferencing_eval\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "_PRIMARY_ADDITIONAL_METRICS = {\n",
        "    \"pdb\": {\n",
        "        \"reconstruct_protein_amino_acid_sequence_0_shot\": {\n",
        "            pdb_reconstruction_eval\n",
        "        },\n",
        "    },\n",
        "    \"biogr\": {\n",
        "        \"georeference_image_0_shot\": {\n",
        "            biodiversity_georeferencing_eval\n",
        "        }\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "_LLM_LIST = [\"command-r-plus\", \"longllama\", \"mixtral-gcp\",\n",
        "             \"gemini-1.5-flash-latest\", \"gemini-1.0-pro\", \"gemini-1.5-pro-latest\",\n",
        "             \"gpt-4o\", \"claude-3-opus-20240229\", 'gemini-2.0-flash-latest']\n",
        "\n",
        "_BIOGR_EXCLUDE_LLM = [\"command-r-plus\", \"longllama\", \"mixtral-gcp\"]\n",
        "_TASK_EVAL_CONFIGS = {\n",
        "    \"hfe\": {\n",
        "        \"extract_hamiltonian_0_shot\": {\n",
        "        }\n",
        "    },\n",
        "    \"hfd\": {\n",
        "        \"derivation_prompt\": {\n",
        "        }\n",
        "    },\n",
        "    \"qecc_65\": {\n",
        "        \"describe_code_in_paper\": {\n",
        "        }\n",
        "    },\n",
        "    \"pdb\": {\n",
        "        \"reconstruct_protein_amino_acid_sequence_0_shot\": {\n",
        "        }\n",
        "    },\n",
        "    \"mpve\": {\n",
        "        \"mat_paper_to_property_1_shot\": {\n",
        "        },\n",
        "        \"mat_paper_to_property_1_shot_bandgap_refractive\": {\n",
        "        },\n",
        "        \"mat_paper_to_property_1_shot_exclude_trivia\": {\n",
        "        },\n",
        "    },\n",
        "    \"dft\": {\n",
        "        \"write_code_for_paper_0_shot\": {\n",
        "        },\n",
        "        \"extract_structure_data_1_shot\": {\n",
        "        },\n",
        "        \"extract_dft_metadata_1_shot\": {\n",
        "        },\n",
        "    },\n",
        "    \"geo\": {\n",
        "        \"extract_dataset_from_geo_papers_0_shot\": {\n",
        "        }\n",
        "    },\n",
        "    \"biogr\": {\n",
        "        \"georeference_image_0_shot\": {\n",
        "        }\n",
        "    },\n",
        "}\n",
        "all_ids_per_task = {'pdb': ['1A12', '1A33', '1AIL', '1AOA', '1AQA', '1AZS', '1BL8', '1BM8', '1BXO', '1CC5', '1CDK', '1CGF', '1CLL', '1CTF', '1DIN', '1DXX', '1E9H', '1EST', '1F9V', '1G6X', '1GOF', '1GP1', '1HCG', '1HHO', '1HNF', '1HNV', '1IAV', '1IGM', '1IGT', '1JM1', '1KXQ', '1M03', '1M17', '1M8Q', '1MBG', '1MBO', '1MHC', '1NKO', '1POH', '1PRC', '1R09', '1RCP', '1RGS', '1SBT', '1SU4', '1TIT', '1TNK', '1UBQ', '2A99', '2ACE', '2AYN', '2J1N', '2POR', '2R6G', '3ADN', '3C7E', '3LCK', '3R2E', '4CPA', '5CPA', '5HZN', '7B3N', '7L1E', '7V8O'],\n",
        "                    'biogr': ['10212153_1', '260729_2', '531730_1', '556058_2', '563682_1', '564487_1', '575983_2', '578304_2', '583537_1', '585031_1', '587419_1', '590257_1', '591038_1', '592166_1', '592526_1', '592805_4', '594665_1', 'S0048969724009641_1', 'S1470160X22006951_1', 'a_decade_of_submersible_observations_1', 'a_new_species_of_river_1', 'a_preliminary_investigation_of_the_3', 'a_simple_genetic_method_to_1', 'a_small_warm_tributary_provides_1', 'abundance_of_longbilled_curlews_on_1a', 'an_overview_of_marine_biodiversity_3', 'an_overview_of_marine_biodiversity_8', 'assessment_of_ambystomatid_salamander_populations_1', 'assessment_of_potential_recovery_viability_2', 'availability_of_supplemental_corn_for_1', 'barriers_to_gene_flow_in_1', 'baseline_assessments_for_coral_reef_1', 'bat_predation_by_spiders_1', 'biotic_assemblages_of_gelatinous_zooplankton_3', 'bird_monitoring_at_effigy_mounds_1', 'birds_of_the_kilbuck_and_1', 'breeding_population_size_of_the_1', 'ceratonova_shasta_infection_in_lower_1', 'characterization_of_a_developing_recreational_1', 'chewing_lice_of_swan_geese_1', 'comparison_of_endoparasite_abundance_and_1', 'comparisons_of_walleye_fecundity_before_1', 'conservation_genetics_of_the_endangered_1', 'cooccurrence_of_ecologically_similar_species_1', 'deep_vs_shallow_gps_tags_1', 'density_of_axis_deer_in_1', 'despite_regional_variation_gymnorhinus_cyanocephalus_1', 'distribution_abundance_and_breeding_activities_1', 'distribution_and_abundance_of_least_3', 'distribution_morphology_and_karyotype_of_1', 'diurnal_human_activity_and_introduced_1', 'diving_patterns_and_foraging_locations_1', 'dna_barcoding_the_native_flowering_5', 'documentation_of_a_probable_spawning_1', 'ece310733_1', 'energy_density_of_three_prosopium_1', 'environment_affects_sucker_catch_rate_1', 'evaluating_spatial_coverage_of_the_1', 'evening_bats_captured_in_a_1', 'expansion_of_smallmouth_bass_distribution_1', 'extreme_wildlife_declines_and_concurrent_1', 'fecal_genotyping_to_estimate_small_1', 'first_record_of_paronatrema_vaginicola_1', 'fish_predation_by_semiaquatic_spiders_1', 'foraging_ecology_of_southern_sea_1', 'four_centuries_of_change_in_1', 'global_conservation_priorities_for_marine_4', 'habitat_suitability_assessment_for_tule_4', 'habitat_use_and_reproductive_success_1', 'hawaiian_hoary_bat_lasiurus_cinereus_1', 'hiding_in_plain_sight_federally_1', 'high_similarity_in_winter_diet_1', 'impacts_of_the_czu_lightning_1', 'incidental_take_of_giant_sea_7', 'incorporating_expanded_sampling_into_an_1', 'inventory_of_eelgrass_zostera_marina_1', 'jwmg22383_1', 'larval_and_juvenile_longfin_smelt_1', 'leveraging_angler_effort_to_inform_1', 'longterm_occupancy_monitoring_reveals_value_1', 'machine_learning_to_understand_patterns_1', 'macrohabitat_suitability_model_for_the_1', 'macroscale_effects_of_the_monument_1', 'marine_biodiversity_in_the_atlantic_4', 'microhabitat_characteristics_and_management_of_1', 'monitoring_fiveneedle_pine_on_bureau_1', 'monitoring_nesting_waterbirds_for_the_1', 'monitoring_questing_winter_tick_abundance_1', 'movement_patterns_of_two_bat_1', 'natal_contributions_of_kokanee_salmon_1', 'occurrence_of_a_reproducing_wild_3', 'occurrence_of_batrachochytrium_dendrobatidis_in_1', 'onceiconic_pismo_clams_persist_in_1', 'patterns_of_florida_bonneted_bat_2', 'population_and_spatial_dynamics_of_1', 'population_density_and_habitat_selection_2', 'population_genomic_surveys_for_six_1', 'postfire_survival_of_the_threatened_4', 'rangewide_genetic_analysis_of_an_1', 'rapid_population_decline_in_mckays_1', 'recovering_the_lost_potential_of_5', 'relative_influence_of_environmental_factors_1', 'rescuing_and_monitoring_white_sturgeon_2', 'revealing_biases_in_insect_observations_5', 'review_of_considerations_for_restoration_1', 'road_and_highway_undercrossings_as_1', 'roseate_tern_breeding_dispersal_and_1', 's41598022209644_1', 's41598023276709_1', 's4159802334533_1', 'sampling_duration_and_season_recommendations_1', 'sea_level_rise_vulnerability_assessment_1', 'seacliff_bedstraw_galium_buxifolium_patterns_4', 'seasonal_and_spatial_distribution_of_1', 'spatial_relationships_and_mesoscale_habitat_1', 'spatial_variation_in_density_of_1', 'status_and_distribution_of_arroyo_2', 'status_assessment_of_the_endangered_1', 'status_of_landbirds_in_the_1', 'striped_bass_movement_in_a_1', 'syntopy_in_california_redlegged_and_1', 'testing_a_singlevisit_sampling_approach_1', 'the_biodiversity_of_the_mediterranean_5', 'the_first_occurrence_of_the_3', 'the_importance_of_subarctic_intertidal_1', 'the_lion_in_west_africa_1', 'time_series_modeling_of_rainfall_1', 'trace_elements_in_blood_of_1', 'travel_management_planning_for_wildlife_1', 'trends_in_amphibian_occupancy_in_1', 'tricolored_blackbird_survey_methods_1', 'tule_elk_selection_of_surface_1', 'unintended_consequences_of_species_translocations_1', 'us_atlantic_and_gulf_of_1', 'use_of_aerial_distance_sampling_1', 'utilizing_the_timetoevent_framework_to_1', 'validating_a_nonlethal_method_of_1', 'western_purple_martin_progne_subis_1'],\n",
        "                    'geo': ['00000', '14614a88b3e44e601c5cf8f71b5e07ca989beb0b', '213d2232a49507f81b4e17e50de7675c88fbc672', '33b0925f7681f3199a5d075324e7f3c5e33f2c76', '41f20bb04729a55ca9c2eaf579adf3ed5729044b', '54a9885771350f38135f30f43ef874e0a30be07b', '5c37c2aa2e108e17e37c6db29a4e5afe6a811119', '7dc47696eb876d85a3dfc6884f61fa8832d5e5e8', '83a1a10e3a2416e1d93bc3dbb482db4ccb707eda', '850ca33e8c1853c1735da63073ec3910bce91ddc', '9bdabc37e4af91c4fb53e205502204b510e3b972', 'a09b49e5f2c6b818e479bd29343eae9005f8ca26', 'ab6d648944f306fa1e2d275115b94d36478d9d2a', 'b90358f971e19a60c305acff2867b89dd197fdf6', 'c3a3a5a24206a9b38d9f4727f78cc8f323e398b2', 'e57ae1987bce88add50696843c8979456ce55561', 'e88aa0bccedc5f07bfee8f2db7a85351e65ec24a', 'e900993457d4d256cbfbe8a7527b6745f130a98e', 'e9c8932d5fcdf067821f8bf24b7462e5c7f73054'], 'hfd': ['1010.1819', '1106.6060', '1208.0116', '1310.2674', '1508.00296', '1812.04213', '2004.04168', '2008.08998', '2012.04554', '2108.02159', '2110.11330', '2111.01152', '2112.07523', '2308.03843', '2308.07488'], 'hfe': ['1010.1819', '1112.4222', '1202.4956', '1206.0608', '1208.0116', '1212.5363', '1401.2167', '1506.01488', '1507.06420', '1510.06887', '1512.02398', '1601.00996', '1812.04213', '1908.05417', '2007.15166', '2008.08998', '2102.13507', '2108.02159', '2111.09813', '2112.07523', '2206.10024', '2208.07620', '2209.15374', '2210.06674', '2210.08025', '2210.14517', '2302.04864', '2303.09821', '2303.18025', '2306.02127', '2306.12486', '2307.03793', '2307.04307', '2307.07531', '2307.11810', '2308.01997', '2308.03843', '2311.13191'], 'qecc_65': ['1501.07779', '1502.05267', '1503.06237', '1503.08800', '1505.02576', '1602.00008', '1603.04442', '1604.07925', '1703.02973', '1707.02308', '1708.08474', '1709.04471', '1709.08658', '1710.04631', '1712.07666', '1801.05897', '1802.07419', '1805.01474', '1809.09801', '1903.03937', '1906.11394', '1907.09528', '1910.10746', '2003.02717', '2007.09154', '2007.12152', '2008.09495', '2009.03921', '2010.06628', '2106.02649', '2107.02194', '2110.11510', '2112.01446', '2201.07802', '2203.00103', '2203.16534', '2209.11405', '2210.10808', '2210.16957', '2212.09935', '2303.02432', '2303.04798', '2306.11621', '2309.16503', '2311.07679', '2311.08653', '2311.13040', '2312.04522', '2402.07476', 'cond-mat_0010440', 'cond-mat_0607736', 'cond-mat_9707273', 'cs_0509062', 'quant-ph_0008040', 'quant-ph_0210097', 'quant-ph_0502086', 'quant-ph_0605138', 'quant-ph_0701020', 'quant-ph_0702075', 'quant-ph_9703002', 'quant-ph_9705052', 'quant-ph_9711021', 'quant-ph_9711049', 'quant-ph_9810055', 'quant-ph_9906114'], 'dft': ['2023_09_22_01b9cdba467fd7882e42g', '2023_09_22_07b4d66e23971ccb85c0g', '2023_09_22_0ce1b5ea9a8637db5435g', '2023_09_22_109a6cd5d015ce89e7f3g', '2023_09_22_12cb692c6b82606615a6g', '2023_09_22_13bcf90c3ef43f1413deg', '2023_09_22_14ab0a44fc8fc33fa338g', '2023_09_22_14ddd903c0b77b5e50c2g', '2023_09_22_182e0132c513bc81a414g', '2023_09_22_1a3a4803f9d9cec16d38g', '2023_09_22_1af7e6342ebcf1ce3ea5g', '2023_09_22_1b00ed3a142a7a1b2582g', '2023_09_22_1def59bea80d6e9f67ccg', '2023_09_22_22c19cfc32d2575f9a52g', '2023_09_22_24d7d9ed97e042af9f29g', '2023_09_22_2d35eebe2e85cecf4103g', '2023_09_22_2d44dda253969d6ce7f6g', '2023_09_22_2e5b7c3f50b6a643e33ag', '2023_09_22_2e9b1b9bffe7fd47b18fg', '2023_09_22_39b7d4444fd6ff852b12g', '2023_09_22_3ae1fab1e33569c30b8dg', '2023_09_22_430c3ddffa99af6a2545g', '2023_09_22_433b6bb3bfc2391f7300g', '2023_09_22_48fb54662c601e035a91g', '2023_09_22_49d6cc9c5e7a469afee0g', '2023_09_22_4eeeb89f0a6f52e58610g', '2023_09_22_4ef39bb4116ac1dad9e9g', '2023_09_22_5237e17b3b341fecc9d9g', '2023_09_22_54050a76c33463a8157fg', '2023_09_22_55f975d508230ef05caeg', '2023_09_22_63a752c620bbc784200cg', '2023_09_22_67e874a3c664208e2d2fg', '2023_09_22_6ac063e0fb85cfc10dd0g', '2023_09_22_70326f83ce0dcec87b50g', '2023_09_22_7433a6c7542334063731g', '2023_09_22_77a765fcab6029c666b4g', '2023_09_22_799ee1c298d190145c70g', '2023_09_22_7c5bbe7e076779b790ccg', '2023_09_22_7c76b066edb1f6f53739g', '2023_09_22_7dab45b11a3ede362147g', '2023_09_22_8181c4d6fa78a2ea82dag', '2023_09_22_81b3dfeb3597db5200c5g', '2023_09_22_84af4abb781aeead403eg', '2023_09_22_87d405f182ae3706ea0cg', '2023_09_22_8b46d7b3e561e7f28495g', '2023_09_22_8df0b56e310badc55de3g', '2023_09_22_900c617369212d6bc72fg', '2023_09_22_910aca2a500d3bf9bf47g', '2023_09_22_980121c407cbdaa46afdg', '2023_09_22_984f5b905c02b6f21733g', '2023_09_22_995805c76f676dddab4fg', '2023_09_22_9a007c3865721f379b39g', '2023_09_22_9a591ebf98377fd0ebe2g', '2023_09_22_9e2bc88db643c6ba8aa0g', '2023_09_22_a0271d2dc7b0f2506498g', '2023_09_22_b0501f9057db320b8ad9g', '2023_09_22_b2865949a80ad08a2835g', '2023_09_22_bba019fc933fc84ad347g', '2023_09_22_cb81fee8faa69f4d7078g', '2023_09_22_cc792b66f9a5779f9798g', '2023_09_22_cff3389f103b8f7971d0g', '2023_09_22_d84d81c022c4b2981048g', '2023_09_22_d90e94cbd96e4b6ddb8bg', '2023_09_22_dd9f0f77c116dc99583ag', '2023_09_22_ddfb75e0fb765dc682bbg', '2023_09_22_e06d11a6e698afe5f2d7g', '2023_09_22_e32d0198a1f3dddb5ba2g', '2023_09_22_e69f3d7ce6c4ff487115g', '2023_09_22_e8d1bc2fb9f3dce5f341g', '2023_09_22_edae82c7fbe0c4062118g', '2023_09_22_efbe854b8da1545fbe9bg', '2023_09_22_f752dc9d5ac72657e3f5g', '2023_09_22_f7714e3a468c91c6f56ag', '2023_09_22_f8875eb68affb0a6cb2bg'], 'mpve': ['10222315', '11093908', '11181068', '12841719', '135893324', '137261967', '137362119', '15804005', '17645319', '2837337', '317542', '53384093', '53519111', '55005437', '6183251', '68518', '97574650']}"
      ],
      "metadata": {
        "id": "sTqvJmb_BxBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# get full eval results\n",
        "\n"
      ],
      "metadata": {
        "id": "wH5McmXXBnEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## calculates metrics"
      ],
      "metadata": {
        "id": "mGlcBn9IVDJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set this to True if you want all metrics including LLMSim scores.\n",
        "runs_full_metric = False #@param"
      ],
      "metadata": {
        "id": "NU1APUUhINwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_json = {}\n",
        "for task in _TASK_EVAL_CONFIGS.keys():\n",
        "  results_json[task] = {}\n",
        "  for prompt in _TASK_EVAL_CONFIGS[task]:\n",
        "    results_json[task][prompt] = {}\n",
        "    for llm_name in _LLM_LIST:\n",
        "      results_json[task][prompt][llm_name] = {}\n",
        "      for record_id in all_ids_per_task[task]:\n",
        "        print(f\"task: {task}, prompt: {prompt}, llm_name: {llm_name}, record_id: {record_id}\")\n",
        "        results_json[task][prompt][llm_name][record_id] = {}\n",
        "        print(f\"{task}, {prompt}, {llm_name}, {record_id}\")\n",
        "        gt_path = os.path.join(file_root_path, \"data\", task, \"ground_truth\", record_id + \".json\")\n",
        "        model_response_path = os.path.join(file_root_path, \"inference_outputs\", task, prompt, llm_name, \"success\", record_id + \".json\")\n",
        "        try:\n",
        "          ground_truth_info, model_response, exception_message, inf_prompt = read_task_ground_truth_and_response(gt_path, model_response_path)\n",
        "          ground_truth_info = preprocess_ground_truth(ground_truth_info, task, prompt)\n",
        "        except Exception as e:\n",
        "          print(f\"ERROR: {e}\")\n",
        "          continue\n",
        "        if (not task == 'pdb' and model_response) or (task == 'pdb' and model_response and inf_prompt):\n",
        "\n",
        "          full_additional_metrics = list(_FULL_ADDITIONAL_METRICS[task][prompt]) if task in _FULL_ADDITIONAL_METRICS  and prompt in _FULL_ADDITIONAL_METRICS[task] else []\n",
        "          primary_additional_metrics = list(_PRIMARY_ADDITIONAL_METRICS[task][prompt]) if task in _PRIMARY_ADDITIONAL_METRICS  and prompt in _PRIMARY_ADDITIONAL_METRICS[task] else []\n",
        "          additional_metrics = full_additional_metrics if runs_full_metric else primary_additional_metrics\n",
        "          all_metrics = additional_metrics + _SHARED_METRCS\n",
        "          for metric in all_metrics:\n",
        "            try:\n",
        "              if task == 'pdb' and metric in additional_metrics:\n",
        "                res = metric(model_response, ground_truth_info, inf_prompt)\n",
        "              else:\n",
        "                res = metric(model_response, ground_truth_info)\n",
        "              print(res)\n",
        "              results_json[task][prompt][llm_name][record_id].update(res)\n",
        "            except Exception as e:\n",
        "              print(\"##### ERROR #####\")\n",
        "              print(e)\n",
        "              print(f\"##### skipped {task}, {prompt}, {llm_name}, {record_id} #####\")\n",
        "              continue\n"
      ],
      "metadata": {
        "id": "cIgjlh5IVGlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_json"
      ],
      "metadata": {
        "id": "wKD1HSXD31qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluate one instance"
      ],
      "metadata": {
        "id": "brbppiTpA4Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_name = 'gemini-2.0-flash-latest' #@param\n",
        "prompt = 'mat_paper_to_property_1_shot' #@param\n",
        "task = 'mpve' #@param\n",
        "record_id = '6183251' #@param"
      ],
      "metadata": {
        "id": "oYpDZrX5BFPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = {}\n",
        "gt_path = os.path.join(file_root_path, \"data\", task, \"ground_truth\", record_id + \".json\")\n",
        "model_response_path = os.path.join(file_root_path, \"inference_outputs\", task, prompt, llm_name, \"success\", record_id + \".json\")\n",
        "try:\n",
        "  ground_truth_info, model_response, exception_message, inf_prompt = read_task_ground_truth_and_response(gt_path, model_response_path)\n",
        "  ground_truth_info = preprocess_ground_truth(ground_truth_info, task, prompt)\n",
        "except Exception as e:\n",
        "  raise Exception(e)\n",
        "if (not task == 'pdb' and model_response) or (task == 'pdb' and model_response and inf_prompt):\n",
        "  full_additional_metrics = list(_FULL_ADDITIONAL_METRICS[task][prompt]) if task in _FULL_ADDITIONAL_METRICS  and prompt in _FULL_ADDITIONAL_METRICS[task] else []\n",
        "  primary_additional_metrics = list(_PRIMARY_ADDITIONAL_METRICS[task][prompt]) if task in _PRIMARY_ADDITIONAL_METRICS  and prompt in _PRIMARY_ADDITIONAL_METRICS[task] else []\n",
        "  additional_metrics = full_additional_metrics if runs_full_metric else primary_additional_metrics\n",
        "  all_metrics = additional_metrics + _SHARED_METRCS\n",
        "  for metric in all_metrics:\n",
        "    try:\n",
        "      if task == 'pdb' and metric in additional_metrics:\n",
        "        res = metric(model_response, ground_truth_info, inf_prompt)\n",
        "      else:\n",
        "        res = metric(model_response, ground_truth_info)\n",
        "      result.update(res)\n",
        "    except Exception as e:\n",
        "      raise Exception(e)\n",
        "\n",
        "print(f'###### eval finished ##########\\nresults: \\n{json5.dumps(result, indent=4)}')"
      ],
      "metadata": {
        "id": "XxnU1V-6BDP2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}